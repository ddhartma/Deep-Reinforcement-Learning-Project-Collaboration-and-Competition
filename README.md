[image1]: assets/trained_agents.gif
[image2]: assets/result.png

# Deep Reinforcement Learning Project - Collaboration and Competition

## Content
- [Introduction](#intro)
- [Unity Environment](#unitity_env)
- [Project Overview](#project)
- [Files in the Repo](#files_in_repo)
- [Watch trained Agents](#trained_agents)
- [Setup Instructions](#Setup_Instructions)
- [Acknowledgments](#Acknowledgments)
- [Further Links](#Further_Links)

## Introduction <a name="what_is_reinforcement"></a>
- "Reinforcement learning is **learning** what to do — **how to map situations to actions** — so as **to maximize a numerical reward** signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them." (Sutton and Barto, [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html))
- Deep reinforcement learning refers to approaches where the knowledge is represented with a deep neural network

- This project is part of the Udacity Nanodegree program 'Deep Reinforcement Learning'. Please check this [link](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893?utm_source=gsem_brand&utm_medium=ads_r&utm_campaign=12906460312_c&utm_term=121838875579&utm_keyword=deep%20reinforcement%20udacity_e&gclid=CjwKCAjw-e2EBhAhEiwAJI5jg7Ycb934lFlosCFVpvwKRD_U5ESjMX18faGkkTTUkIyZVJ6yU4HkohoCyfIQAvD_BwE) for more information.

## Unity Environment <a name="unitity_env"></a>
- [Unity Machine Learning Agents (ML-Agents)](https://github.com/Unity-Technologies/ml-agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents.
- Implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games

## Project Overview <a name="project"></a> 
- In this environment, **two agents** control rackets to bounce a ball over a net. 
- **Reward**: 
    - If an agent hits the ball ***over the net***, it receives a **reward of +0.1**. 
    - If an agent lets a ***ball hit the ground or hits the ball out of bounds***, it receives a **reward of -0.01**. 
- **Goal**: Thus, the goal of each agent is to keep the ball in play.
- **Observation space**: consists of ***8 variables*** corresponding to the **position** and **velocity** of the **ball and racket**. Each agent receives its **own, local observation**. 
- **Actions**: ***Two continuous actions*** are available, corresponding to **movement** toward (or away from) the net, and **jumping**.
- The task is **episodic**.
- **In order to solve the environment**: Agents must get an **average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents)**. 
    - Specifically, after each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. 
    - This yields a single score for each episode.

    ```
    INFO:unityagents:
    'Academy' started successfully!
    Unity Academy name: Academy
            Number of Brains: 1
            Number of External Brains : 1
            Lesson number : 0
            Reset Parameters :
            
    Unity brain name: TennisBrain
            Number of Visual Observations (per agent): 0
            Vector Observation space type: continuous
            Vector Observation space size (per agent): 8
            Number of stacked Vector Observation: 3
            Vector Action space type: continuous
            Vector Action space size (per agent): 2
            Vector Action descriptions: , 
    ```
- The task is **episodic**.

## Files in the repo <a name="files_in_repo"></a>
The workspace contains the following files:
- **README.md**: Markdown file, the readme of this repo.
- **Report.md**: Markdown file, a detailed description of the code implementation.
- **requiremens.txt**: txt file containing python packages needed for this repo.  
- **/assets/...**: Folder containing images and gifs needed for README.md and Report.md.
- **/notebooks_python/Tennis.ipynb**: Main notebook file to train both agents based on a DDPG approach.
- **/notebooks_python/Tennis_Trained_Agent.ipynb**: Second notebook file to watch the behaviour of trained agents.
- **/notebooks_python/ddpg_agent.py**: Python file containing the implementation of the deep reinforcement learning agent based on DDPG.
- **/notebooks_python/model.py**: Python file containing the PyTorch Deep Learning models generated by deep neural networks that acts as a function approximators for actor and critic
    - two actor neural networks (local and target) which map states to action values for policy training and
    - two critic neural networks (local and target) which map states to Q-values for action-value training
- **/notebooks_python/checkpoint_actor.pth**: PyTorch state_dict file containing trained weights for the actor networks.
- **/notebooks_python/checkpoint_critic.pth**: PyTorch state_dict file containing trained weights for the critic networks.
- **/notebooks_python/Reacher.app: Downloaded Unity Environment needed to watch a trained agent.
- **/notebooks_python/unity-environment.log**: Log file repoting the last interaction with the environment.


## Watch trained Agents <a name="trained_agents"></a>
- The video below demonstrates the result of trained agents. Both agents learned to play tennis, i.e. **two agents** learned to control rackets to bounce a ball over a net.

    ![image1]

- Below one can see the scoring plot.

    ![image2]

    - **Score**: This is the corresponding maximum scoring value of both agents for each episode.
    - **Moving Average** is the rolling average with a roll length of 20, i.e. the **Moving Average** is calculated from 20 consecutive **Score** values.   
- Starting from **episode 1763** the moving average was always **greater than 0.5**. Therefore the task (moving average score for both agents AND higher than 0.5 for at least 100 episodes) has been **successfully completed at episode 1862**.

## Setup Instructions <a name="Setup_Instructions"></a>
The following is a brief set of instructions on setting up a cloned repository.

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites: Installation of Python via Anaconda and Command Line Interaface <a name="Prerequisites"></a>
- Install [Anaconda](https://www.anaconda.com/distribution/). Install Python - 64 Bit

    ```
    $ conda upgrade conda
    $ conda upgrade --all
    ```

- Optional: In case of trouble add Anaconda to your system path. Write in your Command Line Interface (CLI)
    ```
    $ export PATH="/path/to/anaconda/bin:$PATH"
    ```

### Clone the project <a name="Clone_the_project"></a>
- Open your CLI
- Change Directory to your project older, e.g. `cd my_github_projects`
- Clone the Github Project inside this folder via:
    ```
    $ git clone https://github.com/ddhartma/Deep-Reinforcement-Learning-Project-Collaboration-and-Competition.git
    ```

- Change Directory
    ```
    $ cd Deep-Reinforcement-Learning-Project-Collaboration-and-Competition
    ```

### Create (and activate) a new environment:
- Create a Python 3.6 environment via conda
    ```
    $ conda create --name drlnd python=3.6
    $ conda activate drlnd
    ```

- To install ```requirements.txt``` in the new environment, use **pip** installed within the environment. Install pip first by
    ```
    $ conda install pip
    ```

- Install all packages provided in ```requirements.txt``` (via pip) needed to train and watch a smart agent
    ```
    $ pip install -r requirements.txt
    ```

- Install torch via conda. Please use the [conda command](https://pytorch.org/) adapted to your platform
    ```
    $ conda install pytorch=0.4.0 -c pytorch
      or for example
    $ conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
    ```

- Clone the repository (if you haven't already!), and navigate to the python/ folder. Then, install several dependencies.
    ```
    git clone https://github.com/udacity/deep-reinforcement-learning.git
    cd deep-reinforcement-learning/python
    pip install .
    ```

- Create an IPython kernel for the **drlnd** environment.
    ```
    $ python -m ipykernel install --user --name drlnd --display-name "drlnd"
    ```

- Check the environment installation via
    ```
    $ conda list
    ```
### Download the Unity environment
- Select the environment that matches your operating system and download it (Version 2: 20 Agent):
    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)
    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)
    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)
    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)
- Then, place this zipped file into the ...notebooks_python/ folder of this repository, and unzip (or decompress) the file.
- Replace the placeholder **<file_name>** in ```Tennis.ipynb```.
    ```
    env = UnityEnvironment(file_name="<file_name>"
    ```
    with the filepath of the executable Unity evironment file (**path/to/Tennis.app** for example).
- Save the notebook.

### To Start Agent Training
- Navigate via CLI to ```Tennis.ipynb```
- Type in terminal
    ```
    $ jupyter notebook Tennis.ipynb
    ```
- Run each cell in the notebook to train the agent.

### To Watch a Smart Agent
- Navigate via CLI to ```Tennis_Trained_Agent.ipynb```
- Type in terminal
    ```
    $ jupyter notebook Tennis_Trained_Agent.ipynb
    ```
- Run each cell in the notebook to watch two trained agents playing tennis.

## Acknowledgments <a name="Acknowledgments"></a>
* This project is part of the Udacity Nanodegree program 'Deep Reinforcement Learning'. Please check this [link](https://www.udacity.com) for more information.

## Further Links <a name="Further_Links"></a>
Git/Github
* [GitFlow](https://datasift.github.io/gitflow/IntroducingGitFlow.html)
* [A successful Git branching model](https://nvie.com/posts/a-successful-git-branching-model/)
* [5 types of Git workflows](https://buddy.works/blog/5-types-of-git-workflows)

Docstrings, DRY, PEP8
* [Python Docstrings](https://www.geeksforgeeks.org/python-docstrings/)
* [DRY](https://www.youtube.com/watch?v=IGH4-ZhfVDk)
* [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)

Further Deep Reinforcement Learning References
* [Very good summary of DQN](https://medium.com/@nisheed/udacity-deep-reinforcement-learning-project-1-navigation-d16b43793af5)
* [An Introduction to Deep Reinforcement Learning](https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c)
* Helpful medium blog post on policies [Off-policy vs On-Policy vs Offline Reinforcement Learning Demystified!](https://kowshikchilamkurthy.medium.com/off-policy-vs-on-policy-vs-offline-reinforcement-learning-demystified-f7f87e275b48)
* [Understanding Baseline Techniques for REINFORCE](https://medium.com/@fork.tree.ai/understanding-baseline-techniques-for-reinforce-53a1e2279b57)
* [Cheatsheet](https://raw.githubusercontent.com/udacity/deep-reinforcement-learning/master/cheatsheet/cheatsheet.pdf)
* [Reinforcement Learning Cheat Sheet](https://towardsdatascience.com/reinforcement-learning-cheat-sheet-2f9453df7651)
* [Reinforcement Learning Textbook](https://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf)
* [Reinforcement Learning Textbook - GitHub Repo to Python Examples](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
* [Udacity DRL Github Repository](https://github.com/udacity/deep-reinforcement-learning)
* [Open AI Gym - Installation Guide](https://github.com/openai/gym#installation)
* [Deep Reinforcement Learning Nanodegree Links](https://docs.google.com/spreadsheets/d/19jUvEO82qt3itGP3mXRmaoMbVOyE6bLOp5_QwqITzaM/edit#gid=0)

Important publications
* [2004 Y. Ng et al., Autonomoushelicopterflightviareinforcementlearning --> Inverse Reinforcement Learning](https://people.eecs.berkeley.edu/~jordan/papers/ng-etal03.pdf)
* [2004 Kohl et al., Policy Gradient Reinforcement Learning for FastQuadrupedal Locomotion --> Policy Gradient Methods](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf)
* [2013-2015, Mnih et al. Human-level control through deep reinforcementlearning --> DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [2014, Silver et al., Deterministic Policy Gradient Algorithms --> DPG](http://proceedings.mlr.press/v32/silver14.html)
* [2015, Lillicrap et al., Continuous control with deep reinforcement learning --> DDPG](https://arxiv.org/abs/1509.02971)
* [2015, Schulman et al, High-Dimensional Continuous Control Using Generalized Advantage Estimation --> GAE](https://arxiv.org/abs/1506.02438)
* [2016, Schulman et al., Benchmarking Deep Reinforcement Learning for Continuous Control --> TRPO and GAE](https://arxiv.org/abs/1604.06778)
* [2017, PPO](https://openai.com/blog/openai-baselines-ppo/)
* [2018, Bart-Maron et al., Distributed Distributional Deterministic Policy Gradients](https://openreview.net/forum?id=SyZipzbCb)
* [2013, Sergey et al., Guided Policy Search --> GPS](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)
* [2015, van Hasselt et al., Deep Reinforcement Learning with Double Q-learning --> DDQN](https://arxiv.org/abs/1509.06461)
* [1993, Truhn et al., Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf)
* [2015, Schaul et al., Prioritized Experience Replay --> PER](https://arxiv.org/abs/1511.05952)
* [2015, Wang et al., Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)
* [2016, Silver et al., Mastering the game of Go with deep neural networks and tree search](https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search)
* [2017, Hessel et al. Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)
* [2016, Mnih et al., Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)
* [2017, Bellemare et al., A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)
* [2017, Fortunato et al., Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)
* [2016, Wang et al., Sample Efficient Actor-Critic with Experience Replay --> ACER](https://arxiv.org/abs/1611.01224)
* [2017, Lowe et al. Multi-Agent Actor-Critic for MixedCooperative-Competitive Environments](https://papers.nips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf)
* [2017, Silver et al. Mastering the Game of Go without Human Knowledge --> AlphaGo Zero](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf)
* [2017, Silver et al., Mastering Chess and Shogi by Self-Play with aGeneral Reinforcement Learning Algorithm --> AlphaZero](https://arxiv.org/pdf/1712.01815.pdf)
